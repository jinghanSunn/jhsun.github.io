<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- saved from url=(0058)http://appsrv.cse.cuhk.edu.hk/~lqyu/DenseVoxNet/index.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link href="./3d_is_proj_index_files/main.css" rel="stylesheet" media="all">
<meta name="description" content="Automatic 3D Cardiovascular MR Segmentation with Densely-Connected Volumetric ConvNets">
<meta name="keywords" content="Automatic 3D Cardiovascular MR Segmentation with Densely-Connected Volumetric ConvNets">


<title>Volumetric Ultrasound</title>
<style>
#layout-content{
    margin: auto;
    padding: 20px;
    background: #ffffff;
    border-radius: 0px;
    width: 900px;
}

</style>


</head>

<body>

<div id="layout-content" style="margin-top:25px;margin-bottom:50px;">
<h2 id="title" class="auto-style1">Towards Automatic Semantic Segmentation in Volumetric Ultrasound</h2>
<p class="auto-style7" align="center">
    <a href="http://appsrv.cse.cuhk.edu.hk/~xinyang/" target="_blank">Xin Yang</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
	<a href="http://appsrv.cse.cuhk.edu.hk/~lqyu/" target="_blank">Lequan Yu</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
	Shengli Li<sup>2</sup>&nbsp;&nbsp;&nbsp;
	Xu Wang<sup>3</sup>&nbsp;&nbsp;&nbsp;
	Na Wang<sup>3</sup>&nbsp;&nbsp;&nbsp;
	<a href="http://sn.polyu.edu.hk/en/people/academic_staff/#harry.qin" target="_blank">Jing Qin</a><sup>4</sup>&nbsp;&nbsp;&nbsp;
	<a href="https://scholar.google.com.hk/citations?user=8Gd16Z0AAAAJ&hl=en&oi=ao" target="_blank">Dong Ni</a><sup>3*</sup>&nbsp;&nbsp;&nbsp;
	<a href="http://www.cse.cuhk.edu.hk/~pheng/" target="_blank">Pheng Ann Heng</a><sup>1,5</sup>&nbsp;&nbsp;&nbsp;
</p>

<p class="auto-style7" align="center">
	<i>nidong@szu.edu.cn</i>&nbsp;&nbsp;&nbsp;
</p>	

<p class="auto-style7" align="center"><sup>1</sup>The Chinese Univeristy of Hong Kong<br>
    <sup>2</sup>Affiliated Shenzhen Maternal and Child Healthcare Hospital of Nanfang Medical University<br>
	<sup>3</sup>Shenzhen University<br>
    <sup>4</sup>The Hong Kong Polytechnic University<br>
	<sup>5</sup>Shenzhen Institutes of Advanced Technology<br>
	</p>
<!--<p class="auto-style7"  align="center">&nbsp;&nbsp;&nbsp; </p>-->
<p align="left">&nbsp;</p>
<p align="center">

<tbody><tr>
	<td><img width="600px" alt="" src="./3d_is_proj_index_files/whole_pic.png"></td>
</tr>
</tbody>


</p><p class="style2"><strong><span class="auto-style4">Abstract</span></strong></p>
<p align="justify" ,class="auto-style5"> 3D ultrasound is rapidly emerging as a viable imaging modality for routine prenatal examinations. However, lacking of efficient tools to decompose the volumetric data greatly limits its widespread. In this paper, we are looking at the problem of volumetric segmentation in ultrasound to promote the volume-based, precise maternal and fetal health monitoring. Our contribution is threefold. First, we propose the first and fully automatic framework for the simultaneous segmentation of multiple objects, including fetus, gestational sac and placenta, in ultrasound volumes, which remains as a rarely-studied but great challenge. Second, based on our customized 3D Fully Convolutional Network, we propose to inject a Recurrent Neural Network (RNN) to flexibly explore 3D semantic knowledge from a novel, sequential perspective, and therefore significantly refine the local segmentation result which is initially corrupted by the ubiquitous boundary uncertainty in ultrasound volumes. Third, considering sequence hierarchy, we introduce a hierarchical deep supervision mechanism to effectively boost the information flow within RNN and further improve the semantic segmentation results. Extensively validated on our in-house large datasets, our approach achieves superior performance and presents to be promising in boosting the interpretation of prenatal ultrasound volumes. Our framework is general and can be easily extended to other volumetric ultrasound segmentation tasks.</p>

<p class="auto-style5">&nbsp;</p>


<p id="Method" ,="" class="auto-style4"><strong>Method</strong></p>
<table width="100%" align="center">
<tbody><tr>
	<td><img width="860px" alt="" src="./3d_is_proj_index_files/framework.png"></td>
</tr>
<tr>
	<td><p class="auto-style5">Schematic view of our proposed framework. System input is an ultrasound volume. Our customized 3D FCN firstly conducts dense voxel-wise semantic labeling and generates intermediate probability volumes for different classes. The RNN trained with hierarchical deep supervision then explores contextual information within multiple volume channels to refine the semantic labeling. System output are extracted volumes of fetus, gestational sac and placenta.</p></td>
</tr>
</tbody></table>
<p class="auto-style5">&nbsp;</p>


<p id="Results" ,="" class="auto-style4"><strong>Results</strong></p>
<table width="100%">
<tbody><tr>
    <td width="5%"></td>
    <td width="90%">
        <div><img src="./3d_is_proj_index_files/visualization.png" width="800px">
        <p align="justify" class="auto-style5">From left to right: cutaway view of ultrasound volume, cutaway view of complete segmentation to show spatial relationship, volume of fetus, gestational sac and placenta.</p>
        </div>
    </td>
    <td width="5%"></td>
</tr>
</tbody></table>
<p></p>
<br><br><table>
<tbody><tr>
    <td width="5%"></td>
    <td width="90%">
        <div align="left"><img src="./3d_is_proj_index_files/result_a.png" width="100%">
        </div>
    </td>
    <td width="5%"></td>
</tr>


<p id="downloads" ,="" class="auto-style4"><strong>Downloads</strong></p>

<table cellspacing="4" cellpadding="2" border="0" style="width: 90%">
<tbody><tr colspan="2">
	<td align="center" valign="center">
		<img style="padding:0; clear:both; " src="./3d_is_proj_index_files/paper.png" align="middle" alt="Snapshot for paper" class="pdf" width="200">
	</td>
	<td align="left" class="auto-style5">"Towards Automatic Semantic Segmentation in Volumetric Ultrasound", MICCAI 2017<br>Xin Yang, Lequan Yu, Shengli Li, Xu Wang, Na Wang, Jing Qin, Dong Ni, Pheng-Ann Heng<br>
<img alt="" height="32" src="./3d_is_proj_index_files/pdf_icon.gif" width="31">&nbsp;&nbsp;[<a href="https://link.springer.com/chapter/10.1007/978-3-319-66182-7_81">Paper</a>]<br><br> 
</td>
</tr>
</tbody></table>
<br>


</p><p class="style2"><strong><span class="auto-style4">Acknowledgments</span></strong></p>
<p align="justify" ,class="auto-style5"> We would like to acknowledge the great effort from our doctors in discussing and annotating the data. It's lucky for us to be the candidate to realize doctor's ideas in this emerging field.</p>

<p class="auto-style7" align="center">"Live and Let Better Live."<br>
</p>


<p class="auto-style1"><font color="#999999">Last update: Sept., 2017</font></p>

</div>



</body></html>